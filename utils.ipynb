{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ba54f69-d324-43c7-8a41-200e9765d175",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daiweiliu112/.local/lib/python3.10/site-packages/numpy/_core/getlimits.py:545: UserWarning: Signature b'\\x00\\xd0\\xcc\\xcc\\xcc\\xcc\\xcc\\xcc\\xfb\\xbf\\x00\\x00\\x00\\x00\\x00\\x00' for <class 'numpy.longdouble'> does not match any known type: falling back to type probe function.\n",
      "This warnings indicates broken support for the dtype!\n",
      "  machar = _get_machar(dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the notebook is: 12.17 KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import unittest\n",
    "import itertools\n",
    "from IPython.display import display, HTML\n",
    "from matplotlib import cm\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib import ticker\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import matplotlib as mpl\n",
    "import os\n",
    "import copy\n",
    "from pathlib import Path\n",
    "import plotly.express as px\n",
    "import datetime\n",
    "import glob\n",
    "import sys\n",
    "from io import StringIO\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "\n",
    "from patsy import dmatrix\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "font = {'size'   : 10}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "#mpl.rcParams['figure.dpi']= 300\n",
    "\n",
    "from scipy.interpolate import make_interp_spline\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "from ipynb.fs.full.data_prep import causes, mtav_causes, inj_causes, pmrc_sql, mech_defect, mech_failed\n",
    "trend_df_lst = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ae2192b-a53f-4562-b65c-d29da54b51ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "multi_sig_trend_graph: \n",
    "inital call for sig trend analysis. \n",
    "input:\n",
    "        df dataframe: one of the main dataframe - either injury, accident, MTAV or PMRC\n",
    "        analysis_type string: determines which folder the graphs will be deposited in,\n",
    "                              also the type of data the analysis is conducted on.\n",
    "        branch int > 0: Unnecessary variable, if we want single filter we will just specify\n",
    "                        number of branching.\n",
    "        p_threshold float > 0 : the p value threshold for a trend to be significant.\n",
    "        auto_filter: list of predetermined filters, bypass user selection if not None.\n",
    "        start_date datetime: the earliest date to be filtered on df.\n",
    "        end_date datetime: the latest date to be filtered on df\n",
    "\n",
    "output:\n",
    "if branch = 0, the user select one of the filter and function calls sig_trend_graph\n",
    "if branch > 0, the user select more than one filter and function calls apply_next to expand\n",
    "                the filter branch\n",
    "'''\n",
    "def multi_sig_trend_graph(df, analysis_type, branch, p_threshold, auto_filter = None, start_date = \"None\", end_date = \"None\"):\n",
    "    print(\"multi_sig_trend_graph\")\n",
    "    print(f\"analysis_type -> {analysis_type}\")\n",
    "    print(f\"branch -> {branch}\")\n",
    "    print(f\"p_threshold -> {p_threshold}\")\n",
    "    print(f\"current auto_filter ->{auto_filter}\")\n",
    "    print(f\"date range -> {start_date} - {end_date}\")\n",
    "    \n",
    "    ## setting up graph_name_lst to be populate by apply_next later\n",
    "    ## this is the return value of the function\n",
    "    graph_name_lst = []\n",
    "\n",
    "    ## if auto_filter is None, then it is manual input, and branching need to be specified\n",
    "    branch = len(auto_filter) if auto_filter != None else branch\n",
    "\n",
    "\n",
    "    ## if there are no start/end data specified\n",
    "    ## has never happenend in my usual function calls\n",
    "    ## can probably make data range specification mandatory\n",
    "    if start_date == \"None\":\n",
    "        start_date = df[\"Date_actual\"][0]\n",
    "    if end_date == \"None\":\n",
    "        end_date = df[\"Date_actual\"].tail(1).values[0]\n",
    "    \n",
    "    df = df.loc[df[\"Date_actual\"] >= start_date]\n",
    "    df = df.loc[df[\"Date_actual\"] <= end_date]\n",
    "    df = df.drop(columns = [\"Date_actual\"])\n",
    "    \n",
    "              \n",
    "    data = df.copy()\n",
    "    choice_lst = []\n",
    "\n",
    "    ## specifies the number of filteres to be applied on the dataset\n",
    "    br_num = input(\"Number of branching: \") if auto_filter == None else str(len(auto_filter))\n",
    "\n",
    "    ## check if it is manual selection, and if the input is valid.\n",
    "    if ord(br_num) <= 57 and ord(br_num) >= 48 and auto_filter == None:\n",
    "        br_num = int(br_num)\n",
    "        for i in reversed(range(br_num)):\n",
    "            valid = False\n",
    "            while valid == False:\n",
    "                ## given the amount of filters by user, specify which filter\n",
    "                choices = input(\"Select {0} from the list above\".format(i+1))\n",
    "                print(choices)\n",
    "                if choices not in list(data.columns.values):\n",
    "                    print(\"The choice is not valid\")\n",
    "                else:\n",
    "                    index = list(data.columns.values).index(choices)\n",
    "                    choices_tuple = (choices, index)\n",
    "                    \n",
    "                    ## only getting choice name, not adding the tuple\n",
    "                    choice_lst.append(choices_tuple[0])\n",
    "                    valid = True\n",
    "    ## Num of branching = length of list of columns in auto_filter\n",
    "    else:\n",
    "        ## assign choice_lst to the copy of auto_filter so the pop() does not affect the original\n",
    "        choice_lst = auto_filter[:]\n",
    "                    \n",
    "    ## Expand based on branching candidates\n",
    "    ## cause_count_iter on the branches specified\n",
    "    choice_lst_iter = choice_lst.copy()\n",
    "    choice_category = '_'.join(choice_lst.copy())\n",
    "    \n",
    "    ## PMRC Specific - includinh passed, failed, tota; for ratio calculation.\n",
    "    choice_lst_iter.extend([\"Date\", \"passed\", \"failed\", \"pmrc_total\"]) if 'pmrc_total' in list(data.columns.values) else choice_lst_iter.extend([\"Date\", \"count\"])\n",
    "\n",
    "    ## dat_small only contains columns of interest\n",
    "    dat_small = data[choice_lst_iter]\n",
    "    source_df = [dat_small]\n",
    "\n",
    "    ## string of all the filters used for this analysis\n",
    "    filter_str = \"-\".join(choice_lst)\n",
    "\n",
    "    print(f\"apply_next filtering {filter_str}\")\n",
    "    ## applies consecutive/hierarchical filtering based on choice_lst\n",
    "    while choice_lst:\n",
    "        curr_filter =  choice_lst.pop(0)\n",
    "        res = apply_next(source_df, \"filter\", curr_filter = curr_filter) ## --- source_df in first call is a dataframe\n",
    "                                                                         ## --- call > 1, its a list of dataframes\n",
    "        source_df = res\n",
    "\n",
    "    ## source_df[0] b/c apply_next filter puts a extra list on top for some reason\n",
    "    source_df = source_df[0]\n",
    "\n",
    "    print(f\"apply_next pivoting {filter_str}\")\n",
    "    ## pivot table so date is index with old row values as columns, count of old row values as new row values.\n",
    "    ## applies to each of the \n",
    "    pivot_df = apply_next(source_df, \"pivot\", start_date = start_date, end_date = end_date)\n",
    "\n",
    "    \n",
    "    print(f\"apply_next graphing {filter_str}\")\n",
    "    #print(\"pivot df being fed to apply next graph\")\n",
    "    #print(pivot_df)\n",
    "    graph_name_lst = apply_next(pivot_df, \"graph\", graph_type = analysis_type, graph_cat = choice_category, graph_name_lst = graph_name_lst)\n",
    "\n",
    "    return (graph_name_lst, filter_str)\n",
    "    \n",
    "\n",
    "        \n",
    "                        \n",
    "            \n",
    "                \n",
    "        \n",
    "        \n",
    "'''\n",
    "apply_next:\n",
    "Does recursive call to filter/pivot hierarchically\n",
    "input:\n",
    "        source_df : df that has been shortened to only include columns of interest.\n",
    "        apply_type: one of filter/pivot/graph.\n",
    "        curr_filter: only when apply_type == filter else None, the current filter being applied in this recursion call\n",
    "        graph_type: only when apply_type == graph else None, one of accident/injuries/MTAV/PMRC. determines save location of graph\n",
    "        graph_cat: only when apply_type == graph else None, appended string of the hierarchical filters to track the filters(columns) used\n",
    "        start_date / end_date: only when apply_type == pivot, specifies the date range which will be used as index in the pivot\n",
    "        graph_name_lst: only when apply_type == graph, collect the graph_cat of every hieriarchical combination of the filters\n",
    "\n",
    "output:\n",
    "        if apply_type == filter: returns source_df that is a layered list of dataframes. Where the layer of list corresponds to the number o hierarchical filters\n",
    "        if apply_type == pivot: returns a pivoted dataframe with data as index, grouped by row values of source_df's columns\n",
    "        if apply_type == graph: returns a trend analysis graph given by the pivoted df of apply_next(pivot)\n",
    "        \n",
    "'''\n",
    "def apply_next(source_df, apply_type, curr_filter = \"None\", graph_type = \"None\", graph_cat = \"None\", start_date = \"None\", end_date = \"None\", graph_name_lst = [],):\n",
    "    if apply_type == \"pivot\":\n",
    "        for i in range(len(source_df)):\n",
    "            if type(source_df[i]) == list:\n",
    "                apply_next(source_df[i], \"pivot\", start_date = start_date, end_date = end_date)\n",
    "            else:\n",
    "                ## at the last level of the filter, cause_count2 will go through every column and pivot.\n",
    "                ## the node level df has uniform values for all columns, so cause_coun2 will just be creating the same pivot df with different column names\n",
    "                ## this is creating new column with all of the column's name merged, so we know which filter we are currently in\n",
    "                df = source_df[i].copy()\n",
    "\n",
    "                ## pmrc combo name position is different than inj and accident\n",
    "                pmrc_combo = df[df.columns[:-4]].apply(lambda x: \"_\".join(x.dropna().astype(str)), axis = 1)\n",
    "                other_combo = df[df.columns[:-2]].apply(lambda x: \"_\".join(x.dropna().astype(str)), axis = 1)\n",
    "                df[\"combo_name\"] = pmrc_combo if 'pmrc_total' in list(source_df[i].columns.values) else other_combo\n",
    "                \n",
    "                source_df[i][\"combo_name\"] = df[\"combo_name\"]\n",
    "\n",
    "                ## passed, failed, total needs to be included bc we need to calculate the ratio in the pivot step\n",
    "                ## WILL NEED TO BE REMOVED LATER\n",
    "                pmrc_bool = True if 'pmrc_total' in source_df[i].columns.values else False\n",
    "                source_df[i] =  source_df[i][[\"Date\", \"passed\", \"failed\",\"pmrc_total\", \"combo_name\"]] if pmrc_bool else source_df[i][[\"Date\", \"count\", \"combo_name\"]]\n",
    "\n",
    "\n",
    "                #print(\"Currently Pivoting {}\".format(combo_name))\n",
    "\n",
    "                ## PMRC Failure Threshold\n",
    "                source_df[i] = find_sig(cause_count2(source_df[i], \"month\", start_date, end_date), 25) if pmrc_bool else find_sig(cause_count2(source_df[i], \"month\", start_date, end_date), 12)\n",
    "        return source_df\n",
    "        \n",
    "                \n",
    "    elif apply_type == \"filter\" and curr_filter != \"None\":\n",
    "        for i in range(len(source_df)):\n",
    "            ## IF ITS LIST, RECURSE UNTIL HITS A DATAFRAME\n",
    "            if type(source_df[i]) == list:\n",
    "                apply_next(source_df[i], \"filter\", curr_filter)\n",
    "            ## REACHED LAST LEVEL - DATAFRAME. USE EVERY UNIQUE ELEMENT OF COLUMN(CURR_FILTER) AS FILTER.\n",
    "            ## CREATING A LIST OF FILTERED DATAFRAME FROM CURRENT DATAFRAME\n",
    "            else:\n",
    "                source_df[i] = iter_filter(source_df[i], curr_filter, 12)\n",
    "\n",
    "        return source_df\n",
    "                \n",
    "    elif apply_type == \"graph\" and graph_type != \"None\":\n",
    "        for i in range(len(source_df)):\n",
    "            if type(source_df[i]) == list:\n",
    "                ## apply_next graph is being called here too in recursion, it is not onlg being called once per graph\n",
    "                apply_next(source_df[i], \"graph\", graph_type = graph_type, graph_cat = graph_cat, graph_name_lst = graph_name_lst)\n",
    "            else:\n",
    "                combo_name = '_'.join(list(source_df[i].columns.values))\n",
    "\n",
    "                ## sig_trend_graph determines if current dataset is significant and returns string\n",
    "                ## graph_name_lst appends the returned string\n",
    "                ## graph_name_lst declared once in multi_trend_graph\n",
    "                ##                repeatedly called in apply_next graph\n",
    "   \n",
    "                graph_name_lst.append(sig_trend_graph(source_df[i], 0.1, graph_type, graph_cat, combo_name = combo_name))\n",
    "\n",
    "        if (graph_name_lst != []):\n",
    "            return graph_name_lst\n",
    "                \n",
    "    \n",
    "    else:\n",
    "        print(\"Type not supported\")\n",
    "\n",
    "    \n",
    "\n",
    "'''\n",
    "compare_result:\n",
    "input:\n",
    "        curr(lst): list of sig or inisg graphs of current period\n",
    "        prev(lst): list of sig or insig graphs of previous period\n",
    "        analysis_type(str): one of accident/injury/MTAV/PMRC\n",
    "        filters: list of filters being used for the graph\n",
    "'''\n",
    "def compare_result(curr, prev, analysis_type, filters):\n",
    "\n",
    "    diff = list(set(curr) - set(prev))\n",
    "    \n",
    "    curr_df = (pd.DataFrame(curr, columns = ['name'])\n",
    "               .assign(category = analysis_type)\n",
    "               .assign(filters = filters)\n",
    "             )\n",
    "    \n",
    "    curr_df = (curr_df\n",
    "               .assign(significant = np.where(curr_df['name'].str[:2] == 'in', \"No\", \"Yes\"))\n",
    "               .assign(trend = np.where(curr_df['name'].str[:10].str.contains('up'), \"up\", \"down\"))\n",
    "               .assign(new_trend = np.where(curr_df['name'].isin(diff), \"Yes\", \"No\"))\n",
    "               .assign(country = np.where(curr_df['name'].str[:17].str.contains('Canada'), \"Canada\", \"USA\"))\n",
    "              )\n",
    "\n",
    "    return curr_df\n",
    "\n",
    "\n",
    "'''\n",
    "auto_filter:\n",
    "takes a list of filters(columns) and apply the trend analysis process\n",
    "input:\n",
    "        lst: list of lists, where each list contains the filters of each call\n",
    "        ret_lst: returned list of dataframe, each dataframe is from compare_result\n",
    "        source_df: Dataframe produced from original enablon export of the csv files\n",
    "        analysis_type_curr: current quarter's folder name which the graphs will be saved to. \n",
    "        analysis_type_prev: previous quarter's folder name which the graphs will be saved to. \n",
    "\n",
    "Output:\n",
    "        populates ret_lst. which is a global list that keeps track of all of the trend analysis runs.\n",
    "'''\n",
    "def auto_filter(lst, ret_lst, source_df, analysis_type_curr, analysis_type_prev, start_date, end_date):\n",
    "    curr_end_date = end_date\n",
    "    prev_end_date = end_date - relativedelta(months = 3)\n",
    "    for filters in lst:\n",
    "        curr_name_lst, filter_str = multi_sig_trend_graph(source_df, analysis_type_curr, 1, 0.1, start_date = start_date, end_date = curr_end_date, auto_filter = filters)\n",
    "        prev_name_lst, filter_str = multi_sig_trend_graph(source_df, analysis_type_prev, 1, 0.1, start_date = start_date, end_date = prev_end_date, auto_filter = filters)\n",
    "        ret_lst.append(compare_result(curr_name_lst, prev_name_lst, analysis_type_curr, filter_str))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "test_df checks if the sum of each categories of the modified df still adds up to the orignal\n",
    "input: \n",
    "      source_df: the dataframe created from reading the original csv file\n",
    "      mod_df: the dataframe returned from cause_count2\n",
    "'''\n",
    "def test_df(source_df, mod_df):\n",
    "    for i in mod_df:\n",
    "        col_name = i.columns.name\n",
    "        check_lst = i.columns.values\n",
    "        total_lst = []\n",
    "        for name in check_lst:\n",
    "            check_df = source_df[source_df[col_name] == name].reset_index(drop = True)\n",
    "            check_num = len(check_df)\n",
    "            test_num = int(sum(i[name]))\n",
    "            if test_num == check_num:\n",
    "                print(\"pass\")\n",
    "            else:\n",
    "                print(\"{0} from column {1} is not correct\".format(name, col_name))\n",
    "                raise ValueError('Total Values Does Not Match')\n",
    "                \n",
    "\n",
    "\n",
    "'''\n",
    "date_as_zero(df, lst) : quantifies the non-existing month occurrence with zero.\n",
    "                        solves the problem of missing months in visualization.\n",
    "input: \n",
    "        df - a single dataframe that is an ouput of cause_cause2\n",
    "        lst - a date template from 2020/01 - 2024/06\n",
    "ouput:\n",
    "        df - a sorted dataframe with consecutive months as index\n",
    "'''\n",
    "def date_as_zero (df, lst):\n",
    "    print(\"Creating zero date placeholder...\")\n",
    "    for i in lst:\n",
    "        if (i not in list(df.index)):\n",
    "            df.loc[i] = [0] * len(df.columns.values)\n",
    "    return df.sort_index()\n",
    "    \n",
    "'''\n",
    "iter_filter(dataframe, string, int):\n",
    "helper function for apply_next type == filter\n",
    "\n",
    "input: dataframe(df) - is an unfiltered df\n",
    "       colname(Str) - current column to be filtered on the dataframe\n",
    "       signum(Int) - the number of occurrence deemed to be significant\n",
    "       \n",
    "output: a list of dataframes, where each dataframe has been filtered by a value in the column.\n",
    "'''\n",
    "def iter_filter(df, colname, signum):\n",
    "    lst_df = []\n",
    "\n",
    "    idx = df[colname].drop_duplicates().reset_index(drop = True)\n",
    "    for i in idx:\n",
    "\n",
    "        ## isolate each element of the filter group\n",
    "        filtered_df = df[df[colname] == i].reset_index(drop = True)\n",
    "        \n",
    "        lst_df.append(filtered_df)\n",
    "        \n",
    "    return lst_df\n",
    "    \n",
    "\n",
    "\n",
    "'''\n",
    "cause_count2(df):\n",
    "helper function for apply_next type == pivot\n",
    "\n",
    "input: \n",
    "        df - the causes dataframe created during the data cleaning step\n",
    "\n",
    "output:\n",
    "        returns a list of dataframes where:\n",
    "            1. each element of the list is a level of causes\n",
    "            2. each df has type of causes as columns, date (2021/01 - 2022-12) as rows.\n",
    "            3. each df's values are count of occurrences of that type of cause during that date.\n",
    "\n",
    "Note: compared to case_count, this function better utilizes pandas functionality with shorter and cleaner code.\n",
    "'''\n",
    "def cause_count2 (df, period, start_date, end_date):\n",
    "    \n",
    "    date_template = pd.date_range(start_date,end_date, \n",
    "              freq='MS').strftime(\"%Y-%m\").tolist()\n",
    "\n",
    "    col_lst = list(df.columns.values)\n",
    "    col_len = len(df[\"Date\"]\n",
    "                  .copy()\n",
    "                  .drop_duplicates()) ## getting how many months the date range covers.\n",
    "    \n",
    "    ## col_lst will only include user selected filters, which one be used as the group and pivot\n",
    "    rm_lst = ['passed', 'failed', 'pmrc_total', 'count', 'Date', 'Quarter']\n",
    "    col_lst = [x for x in col_lst if x not in rm_lst]\n",
    "    \n",
    "    grouped_causes = []\n",
    "\n",
    "    if 'pmrc_total' in list(df.columns.values):\n",
    "        for i in col_lst: ## going through the user selected filters\n",
    "\n",
    "            grouped = (df\n",
    "                        .sort_values([i, \"Date\"]).reset_index(drop = True)\n",
    "                        .groupby(by=[i, \"Date\"], as_index = False).agg({\n",
    "                            'passed': 'sum',\n",
    "                            'failed':'sum',\n",
    "                            'pmrc_total' : 'sum'\n",
    "                        })\n",
    "                        .reset_index()\n",
    "                        .assign(passed_rate = lambda x : x['passed'] / x['pmrc_total'] * 100)\n",
    "                        .assign(failed_rate = lambda x : x['failed'] / x['pmrc_total'] * 100)\n",
    "                        .fillna(0)\n",
    "                        .pivot(index = \"Date\", columns = i, values = 'failed_rate')\n",
    "                      )\n",
    "\n",
    "            grouped_causes.append(grouped) if not grouped.empty else print(\"{} is a empty pivot\".format(i))\n",
    "    else:\n",
    "        for i in col_lst: ## going through user selected filter\n",
    "            grouped = (df[[i, \"Date\", \"count\"]]\n",
    "                        .sort_values([i, \"Date\"]).reset_index(drop = True)\n",
    "                        .groupby(by=[i, \"Date\"], as_index = False).sum()\n",
    "                        .pivot(index = \"Date\", columns = i, values = 'count')\n",
    "                        .fillna(0))\n",
    "            grouped_causes.append(grouped) if not grouped.empty else print(\"empty pivot\")\n",
    "            \n",
    "        \n",
    "    ## None occurrence as rows of zeroes\n",
    "    for i in range(len(grouped_causes)):\n",
    "        \n",
    "        if 'LCR' in grouped_causes[i].columns[0] or 'Efficiency Test' in grouped_causes[i].columns[0]:\n",
    "            print(grouped_causes[i].columns[0])\n",
    "            print(\"PMRC Ratios, no date as zero\")\n",
    "        else:\n",
    "            grouped_causes[i] = date_as_zero(grouped_causes[i], date_template)\n",
    "    \n",
    "        \n",
    "    combined_df = pd.concat(grouped_causes) if grouped_causes != [] else print(\"{} is a empty pivot\".format(i))\n",
    "\n",
    "    return grouped_causes\n",
    "    \n",
    "    \n",
    "'''\n",
    "find_sig(lst, int) : \n",
    "given a significance parameter, remove all columns of data that is below it.\n",
    "\n",
    "input: \n",
    "        cumu_df: list of dataframes from apply_next pivot\n",
    "        sig: int, determines the cut off point for significant data points\n",
    "\n",
    "output: list of dataframes of less or equal len of input\n",
    "'''\n",
    "def find_sig(cumu_df, sig):\n",
    "    print(\"Removing insiginificant columns from list of pivoted df\")\n",
    "    \n",
    "    cumu_copy = cumu_df.copy()\n",
    "    dropped_lst = []\n",
    "    master_df = pd.DataFrame()\n",
    "\n",
    "    ## iterating through each df in the list of df\n",
    "    ## each df has dim[# of months in date range:# of unique element of unpivoted column]\n",
    "    for idx in range(len(cumu_copy)):\n",
    "        df = cumu_copy[idx]\n",
    "        ## iterating each column of the df\n",
    "        for cols in df.columns.values:\n",
    "\n",
    "            ## if it is PMRC, we will look at the average fail%\n",
    "            ## if it other types, we are looking at total count\n",
    "            ## set sig as 5% if its LCR \n",
    "            compare_val = np.average(df[cols]) if df[cols].dtype == float else sum(df[cols])\n",
    "            sig_tmp = 10 if \"LCR\" in cols else sig\n",
    "        \n",
    "            ## if less than sig -> not enough data, df drops the column\n",
    "            print(\"ratio compare_val due to audits or PMRC\") if df[cols].dtype == float else print(\"Count based compare_val for accident/injuries/MTAV\")\n",
    "            print(\"Here is the value form compare_val\")\n",
    "            print(compare_val)\n",
    "            print(\"Here is the value form sig\")\n",
    "            print(sig)\n",
    "            print(\"Here is sig_tmp\")\n",
    "            print(sig_tmp)\n",
    "            print(\"combo_name\")\n",
    "\n",
    "            if compare_val < sig_tmp or len(df[cols]) <= 10:\n",
    "                print(\"This is len of df[cols]\")\n",
    "                print(len(df[cols]))\n",
    "                print(\"{0} Dropped\".format(cols))\n",
    "                df = df.drop(columns = cols)\n",
    "            else:\n",
    "                print(\"nothing was dropped\")\n",
    "                \n",
    "        ## keep track of the dropped columns\n",
    "        dropped_lst.append(cols)\n",
    "\n",
    "        ## replace the original df with the signified df\n",
    "        cumu_copy[idx] = df\n",
    "        \n",
    "        ## in case we dont want it in a list of dataframe structure, but with everything in one df\n",
    "        master_df = pd.concat([master_df, df], axis = 1)\n",
    "\n",
    "    ## after removing insignificant columns, spot and remove any empty df\n",
    "    empty_df_idx = []\n",
    "    for i in reversed(range(len(cumu_copy))):\n",
    "        if cumu_copy[i].empty:\n",
    "            empty_df_idx.append(i)\n",
    "    for i in empty_df_idx:\n",
    "        del(cumu_copy[i])\n",
    "\n",
    "    #print(\"result from find_sig\")\n",
    "    #print(cumu_copy)\n",
    "    return cumu_copy\n",
    "\n",
    "        \n",
    "\n",
    "'''\n",
    "clean_str(str)\n",
    "special characters causes error when putting it through linear regression function\n",
    "treating the accident/injuries/MTAV cause names.\n",
    "\n",
    "input: \n",
    "        str: a string which is the name of the data\n",
    "output:\n",
    "        changes all other ascii character aside from letters and numbers into _. \n",
    "        to avoid errors from the sm.ols linear regression model\n",
    "'''\n",
    "def clean_str(text):\n",
    "    print(\"cleaning graph string names...\")\n",
    "    replace1 = list(range(32, 48))\n",
    "    replace2 = list(range(58, 65))\n",
    "    replace3 = list(range(91,97))\n",
    "    \n",
    "    ## there may be occurrence where the ASCII code is out of bound\n",
    "    text = str(text)\n",
    "    for i in text:\n",
    "        if ord(i) > 127:\n",
    "            text=text.replace(i, \"_\")\n",
    "    \n",
    "    replace_lst = replace1 + replace2 + replace3\n",
    "    repl_chars = ''.join(chr(i) for i in replace_lst)\n",
    "    for c in repl_chars:\n",
    "        text = text.replace(c, \"_\")\n",
    "        \n",
    "    if ord(text[0]) in list(range(48,57)):\n",
    "        first_char = chr(ord(text[0]) + 17)\n",
    "        text = first_char + text[1:]\n",
    "        return text\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "'''\n",
    "sig_trend_graph(df, int, str, str)\n",
    "\n",
    "\n",
    "input: \n",
    "        df: dataframe that has been processed by cause_count (Pivoted) \n",
    "            with dim[# of months in date range: # of unique elements from unpivoted column]\n",
    "        p_threshold: p-value boundary that distinguishes between significance and non-significance\n",
    "        analysis_name: the name of the data which the graph is being produced from\n",
    "        \n",
    "        \n",
    "Usage: used to produce a single graph. All filters must be already be applied to the df. \n",
    "'''\n",
    "\n",
    "def sig_trend_graph(df, p_threshold, analysis_type, analysis_cat, combo_name = \"None\"):\n",
    "    \n",
    "    #print(df)\n",
    "    print(f\"Graphing...{combo_name}\")\n",
    "\n",
    "    data = df.copy()\n",
    "    \n",
    "    for colname in df.columns.values:\n",
    "        data = df.copy()\n",
    "        q_data = df.copy()\n",
    "\n",
    "        ## clean_str changes special characters into _ to prevent bugs from OLS\n",
    "        new_colname = clean_str(colname)\n",
    "        data = data.rename(columns={colname:new_colname})\n",
    "\n",
    "        \n",
    "        y_name = new_colname\n",
    "\n",
    "        ## making the df that would be plotted\n",
    "        y = data[new_colname]\n",
    "        data[\"Date\"] = list(data.index)\n",
    "        data[\"Date\"] = data[\"Date\"].apply(lambda x: x[2:])\n",
    "        data[\"Date_num\"] = np.arange(0,len(data['Date']), 1)\n",
    "\n",
    "        \n",
    "        ## fitting a linear regression model \n",
    "        data = data[[new_colname, \"Date\", \"Date_num\"]]\n",
    "        formula =('{0} ~ Date_num'.format(y_name)) ## y_name seems unnecessary here, just use new_colname\n",
    "        model = smf.ols(formula = formula, data = data)\n",
    "        res_model = model.fit()\n",
    "        model_coeff = res_model.summary().tables[1].as_html()\n",
    "        df_summary = pd.read_html(StringIO(model_coeff),header=0,index_col=0)[0]\n",
    "        b_0 = round(df_summary[\"coef\"][\"Intercept\"],2)\n",
    "        b_1 = round(df_summary[\"coef\"][\"Date_num\"],2)\n",
    "        mean = round(np.mean(data[new_colname]),2)\n",
    "        med = round(np.median(data[new_colname]),2)\n",
    "        p_val = round(df_summary[\"P>|t|\"][\"Date_num\"],3)\n",
    "        textstr = '\\n'.join((\n",
    "            \"mean = {0}\".format(mean),\n",
    "            \"median = {0}\".format(med),\n",
    "            \"P-value = {0}\".format(p_val),\n",
    "            \"Slope = {0}\".format(b_1)))\n",
    "        \n",
    "\n",
    "        ## Add Quarter as a column for quarter step graph\n",
    "        quarter_data = (q_data\n",
    "                        .assign(Date = lambda x: pd.to_datetime(x.index))\n",
    "                        .assign(quarters = lambda x: x[\"Date\"].dt.to_period(\"Q\"))\n",
    "                        .groupby(\"quarters\")[colname].mean().reset_index()\n",
    "                       )\n",
    "\n",
    "        ## add quarter to data df\n",
    "        data = (data\n",
    "                .assign(Date = lambda x: pd.to_datetime(x.index))\n",
    "                .assign(quarters = lambda x: x[\"Date\"].dt.to_period(\"Q\"))\n",
    "                .assign(Date = lambda x: x[\"Date\"].dt.strftime(\"%y-%m\"))\n",
    "                )\n",
    "\n",
    "        ## join to give each month of a quarter the same quarter average\n",
    "        ## for plotting a straigt line for all months in a quarter.\n",
    "        data = (pd\n",
    "            .merge(data, quarter_data, on = 'quarters', how = 'left')\n",
    "               )\n",
    "        y_q = data.iloc[:, -1]\n",
    "\n",
    "\n",
    "\n",
    "        #print(\"quarter_data_development\")\n",
    "        #print(quarter_data)\n",
    "\n",
    "        #print(\"data without quarters\")\n",
    "        #print(data)\n",
    "        \n",
    "        x = data[\"Date_num\"]\n",
    "        lowess = sm.nonparametric.lowess\n",
    "        z = lowess(y, x, frac = 0.35)\n",
    "        x_ = z[:,0]\n",
    "        y_ = z[:,1]\n",
    "        fig, ax = plt.subplots(figsize=(20, 6))\n",
    "        ## plotting the points\n",
    "        _ = ax.plot(data[\"Date\"] , y, '-o', mfc='b', alpha = 0.3)\n",
    "        ## plotting line of best fit\n",
    "        _ = ax.plot(data[\"Date_num\"], b_0 + b_1 * np.array(data[\"Date_num\"]), color = 'red')\n",
    "        ## plotting the lowess smoothed trend\n",
    "        _ = ax.plot(x_, y_)\n",
    "        ## plotting the quarterly step\n",
    "        _ = ax.plot(data[\"Date_num\"], y_q, alpha = 0.3)\n",
    "        _ = ax.yaxis.set_major_locator(MaxNLocator(integer = True))\n",
    "        _ = ax.fill_between(\n",
    "            data[\"Date_num\"],\n",
    "            y_q,\n",
    "            color = 'skyblue',\n",
    "            alpha = 0.2\n",
    "        )\n",
    "\n",
    "        _ = ax.set_ylim(0, max(y) + 5)\n",
    "        _ = ax.set_title(y_name, fontsize=20)\n",
    "        _ = ax.set_ylabel('%Failed', fontsize=12) if analysis_type == \"pmrc_py\" else ax.set_ylabel('Count', fontsize=12)\n",
    "        _ = ax.set_xlabel('Date in Months', fontsize=12)\n",
    "        _ = ax.text(0.9, 0.95, textstr, transform=ax.transAxes, fontsize=14,\n",
    "                verticalalignment='top')\n",
    "\n",
    "\n",
    "        ## NEED TO CHANGE this for PMRC where months are not consecutive\n",
    "        for label in ax.xaxis.get_ticklabels()[::2]:\n",
    "            label.set_visible(False)\n",
    "            \n",
    "        \n",
    "        if combo_name != \"None\":\n",
    "            y_name = clean_str(combo_name)\n",
    "        \n",
    "        ## adding slope info to the graph name\n",
    "        slope = \"up\" if b_1 > 0 else \"down\"\n",
    "        \n",
    "        if p_val > p_threshold:\n",
    "            path_str = 'graphs/{0}/{1}'.format(analysis_type,analysis_cat)\n",
    "            pathname = Path(path_str)\n",
    "            pathname.mkdir(parents = True, exist_ok = True)\n",
    "            graph_name = \"insig_{0}_\".format(slope) + y_name\n",
    "            #insig_lst.append(graph_name)\n",
    "            plt.savefig(Path(path_str + \"/\" + graph_name + \".jpg\"), bbox_inches = 'tight')\n",
    "            plt.close()\n",
    "        else:\n",
    "            path_str = 'graphs/{0}/{1}'.format(analysis_type,analysis_cat)\n",
    "            pathname = Path(path_str)\n",
    "            pathname.mkdir(parents = True, exist_ok = True)\n",
    "            graph_name = \"sig_{0}_\".format(slope) + y_name\n",
    "            #sig_lst.append(graph_name)\n",
    "            plt.savefig(Path(path_str + \"/\" + graph_name + \".jpg\"), bbox_inches = 'tight')\n",
    "            plt.close()\n",
    "            \n",
    "        \n",
    "        data.iloc[:, :-2].to_csv(Path(path_str + \"/\" + graph_name + \".csv\"))\n",
    "        return(graph_name)\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "    \n",
    "            \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "133bc8bf-f8da-4f63-8097-74c579af6b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The size of the notebook is: 35.88 KB\n"
     ]
    }
   ],
   "source": [
    "# Get the current notebook's filename\n",
    "notebook_filename = \"utils.ipynb\"  # Replace with your notebook's name\n",
    "\n",
    "# Get the size of the notebook\n",
    "size = os.path.getsize(notebook_filename)\n",
    "\n",
    "# Convert size to kilobytes (KB) for readability\n",
    "size_kb = size / 1024\n",
    "\n",
    "print(f\"The size of the notebook is: {size_kb:.2f} KB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
